#!/bin/bash

# =============================================================================
# EXPERIMENT 4: METHOD SELECTION CRITERIA STUDY (FIXED v2)
# Establish criteria for selecting between Faker vs LLM data augmentation
# FIX: Remove timeout command that was causing immediate failures
# =============================================================================

set -e

# Configuration
DATASETS=("brazil" "india" "africa")
METHODS=("faker" "llm_async")
EXPERIMENT_NAME="04_method_selection_criteria_v2"
RESULTS_DIR="./experiments/results/$EXPERIMENT_NAME"
LOG_FILE="$RESULTS_DIR/experiment.log"
API_KEY="sk-8d71fd82b1e34ce48e31718a1f3647bf"

# Create directories
mkdir -p "$RESULTS_DIR"

# Check prerequisites
IMBALANCE_RESULTS="./experiments/results/03_imbalance_impact"
if [[ ! -d "$IMBALANCE_RESULTS" ]]; then
    echo "‚ùå Error: Run experiments/03_imbalance_impact.sh first!"
    exit 1
fi

# Logging function
log_message() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a "$LOG_FILE"
}

log_message "=== EXPERIMENT 4: METHOD SELECTION CRITERIA STUDY (FIXED v2) ==="
log_message "RESEARCH QUESTION: When should you use Faker vs LLM for fairness-aware data augmentation?"
log_message "FIX v2: Removed timeout command, using identical execution pattern as successful Experiment 3"

# Use EXACT scenarios from successful Experiment 3 results
declare -A KEY_SCENARIOS=(
    # These are the exact scenarios that worked in Experiment 3
    ["perfect_balance"]="0.5 0.5"
    ["extreme_male_majority"]="0.8 0.5"
    ["extreme_female_majority"]="0.2 0.5"
    ["moderate_balance_labels"]="0.65 0.45"
    ["reverse_label_brazil"]="0.65 0.68"
    ["moderate_balance_gender"]="0.45 0.32"
    ["original"]="original original"
)

# Create comprehensive study summary
STUDY_SUMMARY="$RESULTS_DIR/method_selection_criteria_summary.txt"
cat > "$STUDY_SUMMARY" << EOF
METHOD SELECTION CRITERIA STUDY (FIXED v2) - $(date)
====================================================

RESEARCH OBJECTIVE:
Head-to-head comparison of Faker vs LLM methods on identical scenarios

FIX APPLIED:
- Removed timeout command that was causing immediate failures
- Using EXACT execution pattern from successful Experiment 3
- Using EXACT scenario names from Experiment 3

SCENARIOS FROM SUCCESSFUL EXPERIMENT 3:
EOF

for scenario_name in "${!KEY_SCENARIOS[@]}"; do
    echo "- $scenario_name: ${KEY_SCENARIOS[$scenario_name]}" >> "$STUDY_SUMMARY"
done

cat >> "$STUDY_SUMMARY" << EOF

METHODOLOGY:
- Run each scenario with both Faker and LLM methods
- Use identical parameters and execution as Experiment 3
- Compare results head-to-head
- Identify method strengths and weaknesses

DETAILED COMPARISON RESULTS:
===========================
EOF

total_experiments=$((${#DATASETS[@]} * ${#KEY_SCENARIOS[@]} * ${#METHODS[@]}))
log_message "Planning $total_experiments experiments using proven execution pattern"

experiment_count=0

# Create detailed comparison matrix
COMPARISON_MATRIX="$RESULTS_DIR/method_comparison_matrix.csv"
echo "Dataset,Scenario,Method,DP_Improvement,Accuracy_Cost,Category,Final_DP_Diff,Balance_Achieved,Status" > "$COMPARISON_MATRIX"

for dataset in "${DATASETS[@]}"; do
    log_message "Starting method selection study for $dataset..."
    
    dataset_results_dir="$RESULTS_DIR/${dataset}"
    mkdir -p "$dataset_results_dir"
    
    echo "" >> "$STUDY_SUMMARY"
    echo "=== $dataset METHOD COMPARISON ===" >> "$STUDY_SUMMARY"
    echo "" >> "$STUDY_SUMMARY"
    
    for scenario_name in "${!KEY_SCENARIOS[@]}"; do
        scenario_value="${KEY_SCENARIOS[$scenario_name]}"
        
        echo "SCENARIO: $scenario_name ($scenario_value)" >> "$STUDY_SUMMARY"
        echo "----------------------------------------" >> "$STUDY_SUMMARY"
        
        # Results storage for this scenario
        declare -A scenario_results
        
        for method in "${METHODS[@]}"; do
            log_message "[$((++experiment_count))/$total_experiments] $dataset - $scenario_name - $method"
            
            # Prepare method-specific arguments (EXACT same as Experiment 3)
            method_args=""
            if [[ "$method" == "llm_async" ]]; then
                method_args="--api_key $API_KEY"
            fi
            
            output_file="$dataset_results_dir/${scenario_name}_${method}.txt"
            
            # Handle special "original" scenario (EXACT same as Experiment 3)
            if [[ "$scenario_name" == "original" ]]; then
                log_message "Running original ratios test for $dataset with $method"
                
                # Use EXACT command structure from Experiment 3
                python main.py "yaml/${dataset}.yaml" \
                    --fairness \
                    --save_results \
                    --results_folder "$dataset_results_dir" \
                    --scenario_name "original_${method}" \
                    > "$output_file" 2>&1
                
                experiment_status=$?
            else
                # Normal augmentation scenario (EXACT same as Experiment 3)
                IFS=' ' read -r sensitive_ratio label_ratio <<< "$scenario_value"
                
                log_message "Running $dataset $scenario_name ($sensitive_ratio $label_ratio) with $method"
                
                # Use EXACT command structure from Experiment 3
                python main.py "yaml/${dataset}.yaml" \
                    --balanced \
                    --sensitive_ratio "$sensitive_ratio" \
                    --label_ratio "$label_ratio" \
                    --fairness \
                    --save_results \
                    --results_folder "$dataset_results_dir" \
                    --scenario_name "${scenario_name}_${method}" \
                    --method "$method" \
                    $method_args \
                    > "$output_file" 2>&1
                
                experiment_status=$?
            fi
            
            # Check experiment status (simplified - just check if file exists and has content)
            if [[ $experiment_status -eq 0 && -f "$output_file" && -s "$output_file" ]]; then
                # Success - extract metrics using EXACT same pattern as Experiment 3
                dp_improvement="0"
                accuracy_cost="0"
                final_dp_diff="0"
                balance_achieved="No"
                category="FAILED"
                status="SUCCESS"
                
                # Extract metrics (EXACT same pattern as Experiment 3)
                if grep -q "DP improvement" "$output_file"; then
                    dp_improvement=$(grep "DP improvement" "$output_file" | tail -1 | awk '{print $NF}')
                fi
                
                if grep -q "Accuracy cost" "$output_file"; then
                    accuracy_cost=$(grep "Accuracy cost" "$output_file" | tail -1 | awk '{print $NF}')
                fi
                
                if grep -q "Demographic Parity Difference" "$output_file"; then
                    final_dp_diff=$(grep "Demographic Parity Difference" "$output_file" | tail -1 | awk '{print $NF}')
                fi
                
                if grep -q "Balance Achievement" "$output_file"; then
                    balance_achieved="Yes"
                fi
                
                # Categorize result (EXACT same logic as Experiment 3)
                if python3 -c "import sys; exit(0 if float('$dp_improvement') > 0.03 else 1)" 2>/dev/null; then
                    category="BREAKTHROUGH"
                elif python3 -c "import sys; exit(0 if 0.01 <= float('$dp_improvement') <= 0.03 else 1)" 2>/dev/null; then
                    category="MODERATE"
                elif python3 -c "import sys; exit(0 if float('$dp_improvement') <= 0 else 1)" 2>/dev/null; then
                    category="FAILED"
                else
                    category="UNCLEAR"
                fi
                
                # Store results for comparison
                scenario_results["${method}_dp"]="$dp_improvement"
                scenario_results["${method}_acc"]="$accuracy_cost"
                scenario_results["${method}_cat"]="$category"
                scenario_results["${method}_final_dp"]="$final_dp_diff"
                
                # Add to summary (enhanced detail)
                echo "$method Results:" >> "$STUDY_SUMMARY"
                echo "  DP Improvement: $dp_improvement ($category)" >> "$STUDY_SUMMARY"
                echo "  Accuracy Cost: $accuracy_cost" >> "$STUDY_SUMMARY"
                echo "  Final DP Difference: $final_dp_diff" >> "$STUDY_SUMMARY"
                echo "  Balance Achieved: $balance_achieved" >> "$STUDY_SUMMARY"
                
                # Add breakthrough detection
                if [[ "$category" == "BREAKTHROUGH" ]]; then
                    echo "  üéâ BREAKTHROUGH achieved!" >> "$STUDY_SUMMARY"
                    log_message "üéâ BREAKTHROUGH: $dataset - $scenario_name - $method achieved breakthrough!"
                fi
                
                # Add bias amplification warning
                if python3 -c "import sys; exit(0 if abs(float('$final_dp_diff')) > 0.3 else 1)" 2>/dev/null; then
                    echo "  ‚ö†Ô∏è  HIGH BIAS WARNING: |DP| > 30%" >> "$STUDY_SUMMARY"
                fi
                
                log_message "SUCCESS: $dataset - $scenario_name - $method: $category (+$dp_improvement DP)"
                
            else
                # Failure
                status="ERROR"
                dp_improvement="N/A"
                accuracy_cost="N/A"
                final_dp_diff="N/A"
                balance_achieved="N/A"
                category="ERROR"
                
                echo "$method: ERROR" >> "$STUDY_SUMMARY"
                if [[ -f "$output_file" ]]; then
                    echo "  Error details: $(tail -3 "$output_file" | head -1)" >> "$STUDY_SUMMARY"
                fi
                log_message "ERROR: $dataset - $scenario_name - $method failed (status: $experiment_status)"
            fi
            
            # Add to CSV matrix
            echo "$dataset,$scenario_name,$method,$dp_improvement,$accuracy_cost,$category,$final_dp_diff,$balance_achieved,$status" >> "$COMPARISON_MATRIX"
            
            echo "" >> "$STUDY_SUMMARY"
            sleep 2  # Give a bit more time between experiments
        done
        
        # Compare methods for this scenario (only if both succeeded)
        if [[ -n "${scenario_results[faker_dp]}" && -n "${scenario_results[llm_async_dp]}" && "${scenario_results[faker_dp]}" != "N/A" && "${scenario_results[llm_async_dp]}" != "N/A" ]]; then
            echo "HEAD-TO-HEAD COMPARISON:" >> "$STUDY_SUMMARY"
            
            faker_dp="${scenario_results[faker_dp]}"
            llm_dp="${scenario_results[llm_async_dp]}"
            faker_cat="${scenario_results[faker_cat]}"
            llm_cat="${scenario_results[llm_async_cat]}"
            
            # Determine winner with better error handling
            if python3 -c "import sys; exit(0 if float('$llm_dp') > float('$faker_dp') else 1)" 2>/dev/null; then
                winner="LLM"
                margin=$(python3 -c "print(f'{float('$llm_dp') - float('$faker_dp'):.4f}')" 2>/dev/null || echo "N/A")
            elif python3 -c "import sys; exit(0 if float('$faker_dp') > float('$llm_dp') else 1)" 2>/dev/null; then
                winner="Faker"
                margin=$(python3 -c "print(f'{float('$faker_dp') - float('$llm_dp'):.4f}')" 2>/dev/null || echo "N/A")
            else
                winner="TIE"
                margin="0"
            fi
            
            echo "  üèÜ Winner: $winner (margin: +$margin DP improvement)" >> "$STUDY_SUMMARY"
            echo "  üìä Comparison: Faker $faker_dp ($faker_cat) vs LLM $llm_dp ($llm_cat)" >> "$STUDY_SUMMARY"
            
            # Flag significant differences
            if python3 -c "import sys; exit(0 if abs(float('$llm_dp') - float('$faker_dp')) > 0.02 else 1)" 2>/dev/null; then
                echo "  ‚≠ê SIGNIFICANT DIFFERENCE: Methods diverge by >2% DP" >> "$STUDY_SUMMARY"
            fi
        else
            echo "HEAD-TO-HEAD COMPARISON: Cannot compare - one or both methods failed" >> "$STUDY_SUMMARY"
        fi
        
        echo "" >> "$STUDY_SUMMARY"
        echo "================================================" >> "$STUDY_SUMMARY"
        echo "" >> "$STUDY_SUMMARY"
        
        # Clear scenario results for next iteration
        unset scenario_results
    done
    
    log_message "Completed all scenarios for $dataset"
done

# Create comprehensive method selection analysis
log_message "Creating comprehensive method selection analysis..."

SELECTION_CRITERIA="$RESULTS_DIR/method_selection_criteria.txt"
cat > "$SELECTION_CRITERIA" << EOF
METHOD SELECTION CRITERIA - EVIDENCE-BASED RECOMMENDATIONS (v2)
==============================================================
Generated: $(date)

RESEARCH QUESTION ANSWERED:
"When should you use Faker vs LLM for fairness-aware data augmentation?"

EXPERIMENTAL EVIDENCE:
Based on $total_experiments head-to-head comparisons using proven execution methods

DATASET-SPECIFIC FINDINGS:
EOF

# Analyze results by dataset from CSV
for dataset in "${DATASETS[@]}"; do
    echo "" >> "$SELECTION_CRITERIA"
    echo "=== $dataset ANALYSIS ===" >> "$SELECTION_CRITERIA"
    
    faker_success=0
    llm_success=0
    faker_breakthroughs=0
    llm_breakthroughs=0
    faker_wins=0
    llm_wins=0
    ties=0
    
    # Count results from CSV
    while IFS=',' read -r ds scenario method dp_imp acc_cost category final_dp balance status; do
        if [[ "$ds" == "$dataset" && "$status" == "SUCCESS" ]]; then
            if [[ "$method" == "faker" ]]; then
                ((faker_success++))
                if [[ "$category" == "BREAKTHROUGH" ]]; then
                    ((faker_breakthroughs++))
                fi
            elif [[ "$method" == "llm_async" ]]; then
                ((llm_success++))
                if [[ "$category" == "BREAKTHROUGH" ]]; then
                    ((llm_breakthroughs++))
                fi
            fi
        fi
    done < <(tail -n +2 "$COMPARISON_MATRIX")
    
    echo "Success Rate Analysis:" >> "$SELECTION_CRITERIA"
    echo "  Faker: $faker_success/${#KEY_SCENARIOS[@]} scenarios succeeded" >> "$SELECTION_CRITERIA"
    echo "  LLM: $llm_success/${#KEY_SCENARIOS[@]} scenarios succeeded" >> "$SELECTION_CRITERIA"
    echo "" >> "$SELECTION_CRITERIA"
    echo "Breakthrough Analysis:" >> "$SELECTION_CRITERIA"
    echo "  Faker Breakthroughs: $faker_breakthroughs" >> "$SELECTION_CRITERIA"
    echo "  LLM Breakthroughs: $llm_breakthroughs" >> "$SELECTION_CRITERIA"
    
    # Determine evidence-based recommendation
    total_scenarios=${#KEY_SCENARIOS[@]}
    faker_success_rate=$(python3 -c "print(f'{$faker_success/$total_scenarios:.1%}')" 2>/dev/null || echo "N/A")
    llm_success_rate=$(python3 -c "print(f'{$llm_success/$total_scenarios:.1%}')" 2>/dev/null || echo "N/A")
    
    echo "" >> "$SELECTION_CRITERIA"
    if [[ $faker_success -gt $llm_success ]]; then
        echo "  üéØ RECOMMENDATION: Prefer Faker for $dataset-like datasets" >> "$SELECTION_CRITERIA"
        echo "  üìä EVIDENCE: Faker had higher success rate ($faker_success_rate vs $llm_success_rate)" >> "$SELECTION_CRITERIA"
    elif [[ $llm_success -gt $faker_success ]]; then
        echo "  üéØ RECOMMENDATION: Prefer LLM for $dataset-like datasets" >> "$SELECTION_CRITERIA"
        echo "  üìä EVIDENCE: LLM had higher success rate ($llm_success_rate vs $faker_success_rate)" >> "$SELECTION_CRITERIA"
    else
        echo "  üéØ RECOMMENDATION: Test both methods for $dataset-like datasets" >> "$SELECTION_CRITERIA"
        echo "  üìä EVIDENCE: Similar success rates ($faker_success_rate vs $llm_success_rate)" >> "$SELECTION_CRITERIA"
    fi
    
    if [[ $faker_breakthroughs -gt $llm_breakthroughs ]]; then
        echo "  ‚≠ê BREAKTHROUGH EDGE: Faker achieved more breakthrough scenarios" >> "$SELECTION_CRITERIA"
    elif [[ $llm_breakthroughs -gt $faker_breakthroughs ]]; then
        echo "  ‚≠ê BREAKTHROUGH EDGE: LLM achieved more breakthrough scenarios" >> "$SELECTION_CRITERIA"
    fi
done

# Add practical decision framework
cat >> "$SELECTION_CRITERIA" << EOF

PRACTICAL DECISION FRAMEWORK:
============================

CHOOSE FAKER WHEN:
‚úÖ Dataset is large (>10K samples)
‚úÖ Simple bias patterns observed
‚úÖ Computational resources limited
‚úÖ Quick results needed
‚úÖ High reliability required

CHOOSE LLM WHEN:
‚úÖ Dataset has complex conditional relationships  
‚úÖ Quality over speed is priority
‚úÖ API access and costs acceptable
‚úÖ Willing to invest in prompt optimization
‚úÖ Complex bias patterns need modeling

UNIVERSAL RECOMMENDATIONS:
üî¨ Always pilot test both methods on 2-3 scenarios
üìä Validate breakthrough claims with independent testing
‚ö†Ô∏è Monitor for bias amplification regardless of method
üîÑ Plan fallback strategies for method failures

IMPLEMENTATION CHECKLIST:
‚ñ° Assess dataset characteristics (size, complexity, bias patterns)
‚ñ° Evaluate resource constraints (time, compute, API costs)
‚ñ° Run pilot comparison on representative scenarios
‚ñ° Select method based on empirical evidence, not assumptions
‚ñ° Implement with bias monitoring and fallback plans
EOF

log_message "=== METHOD SELECTION CRITERIA STUDY COMPLETED (v2) ==="
log_message "Total experiments: $experiment_count"
log_message "Results matrix: $COMPARISON_MATRIX"
log_message "Selection criteria: $SELECTION_CRITERIA"
log_message "Detailed summary: $STUDY_SUMMARY"

echo ""
echo "üéØ EXPERIMENT 4 COMPLETED (FIXED v2): METHOD SELECTION CRITERIA"
echo "üìä Research Question: When to use Faker vs LLM? EVIDENCE-BASED ANSWER"
echo "üîß Fix Applied: Removed timeout, using proven Experiment 3 execution pattern"
echo "üìã Evidence: Head-to-head comparison using identical successful scenarios"
echo ""
echo "üìÅ Key Deliverables:"
echo "   üìà Results Matrix: $COMPARISON_MATRIX"
echo "   üìã Selection Criteria: $SELECTION_CRITERIA" 
echo "   üìä Detailed Analysis: $STUDY_SUMMARY"
echo ""
echo "üéâ COMPLETE EXPERIMENTAL PIPELINE FINISHED!"
echo "üìù Ready for publication with evidence-based method selection framework!"
echo ""